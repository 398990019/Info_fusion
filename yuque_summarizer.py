# yuque_summarizer.py (ä¿®æ­£ç‰ˆ)

from simhash import Simhash
from diff_utils import find_diff
# --- ä¿®æ­£ 1: å¯¼å…¥æ­£ç¡®çš„è¯­é›€è·å–å‡½æ•° ---
from yuque_fetcher import fetch_yuque_data
import json
import os
import requests
from datetime import datetime, timedelta
from simhash_utils import generate_simhash, get_hamming_distance
# --- ä¿®æ­£ 2: å¯¼å…¥ç»Ÿä¸€é…ç½®ä¸­å¿ƒçš„é…ç½® ---
from config import YUQUE_TOKEN, YUQUE_GROUP, YUQUE_BOOK, YUQUE_BASE_URL, SIMHASH_THRESHOLD


# ----------------------------------------------------------------------
# è¾…åŠ©å‡½æ•°ï¼šæ•°æ®å­˜å‚¨ä¸åŠ è½½
# ----------------------------------------------------------------------

def load_data(filename):
    """ä»æ–‡ä»¶ä¸­åŠ è½½å†å²æ•°æ®ã€‚"""
    if os.path.exists(filename):
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                return json.load(f)

        except json.JSONDecodeError:
            print(f"è­¦å‘Šï¼Œæ— æ³•è§£ææ–‡ä»¶{filename}ï¼Œå°†é‡æ–°åˆ›å»ºã€‚")
            return {}
    else:
        return {}


def save_data(filename, data):
    """å°†æ•°æ®ä¿å­˜åˆ°æ–‡ä»¶ã€‚"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
    except IOError as e:
        print(f"é”™è¯¯ï¼šæ— æ³•å°†æ•°æ®ä¿å­˜åˆ°æ–‡ä»¶{filename}:{e}")


# ----------------------------------------------------------------------
# è¾…åŠ©å‡½æ•°ï¼šè·å–å•ä¸ªæ–‡æ¡£æ­£æ–‡
# ----------------------------------------------------------------------

def get_doc_body(doc_id: str) -> str or None:
    """
    è·å–å•ä¸ªè¯­é›€æ–‡æ¡£çš„æ­£æ–‡å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰ã€‚
    å†…éƒ¨ä½¿ç”¨ config.py ä¸­çš„å…¨å±€é…ç½®ã€‚
    """
    # ä½¿ç”¨ config.py ä¸­çš„é…ç½®
    url = f"{YUQUE_BASE_URL}/repos/{YUQUE_GROUP}/{YUQUE_BOOK}/docs/{doc_id}?raw=true"
    headers = {
        "X-Auth-Token": YUQUE_TOKEN,
        "Content-Type": "application/json"
    }

    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        doc_data = response.json().get("data")

        if doc_data:
            return doc_data.get('body_markdown', doc_data.get('body'))
        return None

    except requests.exceptions.RequestException as e:
        print(f"ERROR: è·å–æ–‡æ¡£ {doc_id} æ­£æ–‡å¤±è´¥: {e}")
        return None


# ----------------------------------------------------------------------
# æ ¸å¿ƒåŠŸèƒ½ï¼šè¯­é›€æ–‡æ¡£å¢é‡æ›´æ–°ä¸å·®å¼‚æ£€æµ‹
# ----------------------------------------------------------------------

def check_yuque_updates(data_file='yuque_history.json', simhash_threshold=SIMHASH_THRESHOLD):
    """
    æ£€æŸ¥è¯­é›€çŸ¥è¯†åº“çš„æ›´æ–°ï¼Œè¿‡æ»¤ä¸æ˜¾è‘—çš„æ›´æ”¹å’ŒåŒè´¨åŒ–å†…å®¹ã€‚

    è¿”å›:
    list: åŒ…å«æœ‰æ˜¾è‘—å˜åŒ–æˆ–æ–°å¢æ–‡æ¡£çš„åˆ—è¡¨ (ä»…åŒ…å«æ–‡æ¡£å…ƒæ•°æ®)ã€‚
    """
    # 1. è·å–æœ¬æ¬¡è¿è¡Œçš„æ–‡æ¡£å…ƒæ•°æ®
    yuque_metadata = fetch_yuque_data()

    if not yuque_metadata:
        print("æ— æ³•è·å–è¯­é›€æ•°æ®ï¼Œè¯·æ£€æŸ¥é…ç½®æˆ–ç½‘ç»œè¿æ¥ã€‚")
        return []

    # 2. åŠ è½½ä¸Šæ¬¡è¿è¡Œçš„å†å²æ•°æ®
    last_run_data = load_data(data_file)
    new_run_data = {}  # ç”¨äºå­˜å‚¨æœ¬æ¬¡è¿è¡Œåçš„æ‰€æœ‰æ•°æ®
    updated_docs_meta = []  # å­˜å‚¨éœ€è¦è¿”å›ç»™ä¸»æµç¨‹è¿›è¡Œ AI å¤„ç†çš„æ–‡æ¡£å…ƒæ•°æ®

    for doc_meta in yuque_metadata:
        doc_id_str = str(doc_meta.get('id'))

        # æå–å…³é”®ä¿¡æ¯
        updated_at = doc_meta.get('updated_at')
        last_doc_info = last_run_data.get(doc_id_str)

        # --------------------------------------------
        # æ­¥éª¤ A: åˆ¤æ–­æ–‡æ¡£æ˜¯å¦æ›´æ–° (æ—¶é—´æˆ³æ¯”å¯¹)
        # --------------------------------------------
        is_updated = False
        if not last_doc_info:
            # æ–°å¢æ–‡æ¡£
            is_updated = True
            print(f" [NEW] æ–°å¢æ–‡æ¡£: {doc_meta.get('title')}")
        elif updated_at > last_doc_info.get('updated_at', '1970-01-01T00:00:00Z'):
            # å·²æœ‰æ–‡æ¡£ï¼Œä¸”æ—¶é—´æˆ³å‘ç”Ÿå˜åŒ–
            is_updated = True
            print(f" [UPDATE] æ–‡æ¡£å·²æ›´æ–°: {doc_meta.get('title')}")

        # --------------------------------------------
        # æ­¥éª¤ B: å¦‚æœå·²æ›´æ–°ï¼Œåˆ™è¿›ä¸€æ­¥åˆ¤æ–­å·®å¼‚æ˜¯å¦æ˜¾è‘— (SimHashæ¯”å¯¹)
        # --------------------------------------------
        if is_updated:
            document_body = get_doc_body(doc_id_str)
            if not document_body:
                # æ— æ³•è·å–æ­£æ–‡ï¼Œè·³è¿‡æœ¬æ¬¡å¤„ç†
                new_run_data[doc_id_str] = last_doc_info or doc_meta  # è‡³å°‘ä¿ç•™å…ƒæ•°æ®
                continue

            # ç”Ÿæˆæ–°æ–‡æ¡£çš„ SimHash
            new_simhash_obj = generate_simhash(document_body)
            new_doc_info = doc_meta.copy()

            # æ£€æŸ¥å·®å¼‚æ˜¯å¦æ˜¾è‘—
            is_significant = False

            if not last_doc_info:
                # æ–°æ–‡æ¡£ï¼šè‡ªåŠ¨è§†ä¸ºæ˜¾è‘—æ›´æ–°
                is_significant = True
            elif last_doc_info.get('simhash'):
                # éé¦–æ¬¡ï¼šæ¯”å¯¹ SimHash æ±‰æ˜è·ç¦»
                old_simhash_value = last_doc_info['simhash']
                old_simhash_obj = Simhash(int(old_simhash_value))

                distance = get_hamming_distance(old_simhash_obj, new_simhash_obj)
                print(f"   - SimHash è·ç¦»: {distance} (é˜ˆå€¼: {simhash_threshold})")

                if distance > simhash_threshold:
                    # è·ç¦»å¤§äºé˜ˆå€¼ï¼Œåˆ¤å®šä¸ºæ˜¾è‘—æ›´æ–°
                    is_significant = True

                    # é¢å¤–è¾“å‡ºå·®å¼‚å†…å®¹ï¼ˆå¯é€‰ï¼Œä»…ä½œè°ƒè¯•/é€šçŸ¥ç”¨ï¼‰
                    # old_doc_body = last_doc_info.get("body", "")
                    # diff_content = find_diff(old_doc_body, document_body)
                    # if diff_content:
                    #     print("   - è¡Œçº§å·®å¼‚å†…å®¹å·²æ£€å‡º...")

            if is_significant:
                # æ˜¾è‘—æ›´æ–°ï¼šåŠ å…¥å¾…å¤„ç†åˆ—è¡¨
                updated_docs_meta.append(doc_meta)
                print("   -> åˆ¤å®šä¸ºæ˜¾è‘—æ›´æ–°/æ–°å¢ï¼Œå°†æäº¤ç»™ LLM é‡æ–°å¤„ç†ã€‚")
            else:
                print("   -> SimHash è·ç¦»è¾ƒè¿‘ï¼Œåˆ¤å®šä¸ºå¾®å°ä¿®æ”¹ï¼Œè¿‡æ»¤ã€‚")

            # æ— è®ºæ˜¯å¦æ˜¾è‘—ï¼Œéƒ½éœ€è¦æ›´æ–°å†å²æ•°æ®ä¸­çš„ 'body' å’Œ 'simhash'
            new_run_data[doc_id_str] = new_doc_info
            new_run_data[doc_id_str]["body"] = document_body
            new_run_data[doc_id_str]["simhash"] = new_simhash_obj.value

        else:
            # 3. æ–‡æ¡£æœªæ›´æ–°ï¼šç›´æ¥å¤åˆ¶æ—§æ•°æ®ï¼Œç¡®ä¿æŒ‡çº¹å’Œæ­£æ–‡ä¸ä¸¢å¤±
            new_run_data[doc_id_str] = last_doc_info.copy()

    # 4. ä¿å­˜æœ¬æ¬¡è¿è¡Œçš„æ•°æ®
    save_data(data_file, new_run_data)
    print(f"\n--- è¯­é›€å¢é‡æ£€æµ‹å®Œæˆï¼š{len(updated_docs_meta)} ç¯‡æ–‡æ¡£éœ€é‡æ–°å¤„ç†ã€‚ ---")

    return updated_docs_meta


# ----------------------------------------------------------------------
if __name__ == "__main__":
    print("--- ğŸ”¬ yuque_summarizer.py å¢é‡æ£€æµ‹æ¨¡å—è‡ªæ£€ ---")

    # è¿è¡Œå¢é‡æ£€æµ‹å‡½æ•°
    significant_updates = check_yuque_updates()

    if significant_updates:
        print("\nä»¥ä¸‹æ–‡æ¡£æœ‰æ˜¾è‘—æ›´æ–°ï¼Œéœ€æäº¤ç»™ LLMï¼š")
        for doc in significant_updates:
            print(f" - {doc.get('title')} (ID: {doc.get('id')})")
    else:
        print("\næœ¬æ¬¡è¿è¡Œæœªæ£€æµ‹åˆ°æ˜¾è‘—æ›´æ–°æˆ–æ–°å¢æ–‡æ¡£ã€‚")