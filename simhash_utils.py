from simhash import Simhash
import jieba
from collections import Counter

# --- æ‰©å±•åœç”¨è¯åˆ—è¡¨ ---
STOP_WORDS = set([
    'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'ä¸', 'è€Œ', 'ä¹‹', 'æ‰€', 'æˆ–', 'éƒ½', 'ä¸', 'æˆ‘', 'ä½ ', 'ä»–',
    'æˆ‘ä»¬', 'ä»–ä»¬', 'ä¸€ä¸ª', 'ä¸€ç§', 'ä¸€äº›', 'å¯ä»¥', 'è¿›è¡Œ', 'å·²ç»', 'å¯¹äº', 'ä½†æ˜¯',
    'å¦‚æœ', 'é€šè¿‡', 'ç›¸å…³', 'ç”±äº', 'æˆä¸º', 'å‘ç°', 'åŒæ—¶', 'åŠå…¶', 'åŸºäº', 'å…·æœ‰',
    'è¿™äº›', 'ä¾‹å¦‚', 'ä¸ä»…', 'åªæ˜¯', 'ä½œä¸º', 'ä¸€ç›´', 'ä»¥æ¥', 'è¿˜æ˜¯', 'æ­£åœ¨',
    'å¹¶', 'å°†', 'è¢«', 'å¯¹', 'ä»', 'ç”±', 'ä¸', 'åŠ', 'è·Ÿ', 'åŒ', 'å’Œ', 'åˆ',
    # é¢†åŸŸä¸­å¸¸è§çš„å†—ä½™è¯
    'å­¦é™¢', 'ç ”ç©¶', 'æ¢ç´¢', 'å‘å±•', 'çŸ¥è¯†', 'æ¨åŠ¨', 'ä¸“æ³¨', 'è‡´åŠ›'
])

# --- åŒä¹‰è¯/å½’ä¸€åŒ–å­—å…¸ ---
NORMALIZE_MAP = {
    "ç ”ç©¶": "æ¢ç´¢",
    "è‡´åŠ›": "ä¸“æ³¨",
    "èåˆ": "ç»“åˆ",
    "å‰æ²¿": "æœ€æ–°",
    "å­¦é™¢": "é™¢ç³»",
    "é¢†åŸŸ": "å­¦ç§‘",
    "æ¨åŠ¨": "ä¿ƒè¿›",
    "å‘å±•": "è¿›æ­¥",
    "è·¨å­¦ç§‘": "å¤šå­¦ç§‘",
    "äººå·¥æ™ºèƒ½": "AI",
    "å“²å­¦": "äººæ–‡å­¦ç§‘",
    "ç¤¾ä¼šå­¦": "äººæ–‡å­¦ç§‘"
}

def normalize_token(token: str) -> str:
    """å°†è¯è¯­åšå½’ä¸€åŒ–ï¼ˆåŒä¹‰è¯æ˜ å°„ï¼‰ã€‚"""
    return NORMALIZE_MAP.get(token, token)

def init_custom_words():
    """
    æ·»åŠ è‡ªå®šä¹‰é¢†åŸŸè¯æ±‡ï¼Œé¿å… jieba é”™åˆ†ã€‚
    """
    custom_words = [
        "äººå·¥æ™ºèƒ½å­¦é™¢",
        "äººå·¥æ™ºèƒ½",
        "è·¨å­¦ç§‘",
        "è®¡ç®—æœºç§‘å­¦",
        "ç¥ç»ç§‘å­¦",
        "ç¤¾ä¼šå­¦",
        "å“²å­¦"
    ]
    for w in custom_words:
        jieba.add_word(w)

def get_tokens(doc_text):
    """
    æ”¹è¿›ç‰ˆåˆ†è¯å‡½æ•°ï¼š
    1. ä½¿ç”¨ jieba åˆ†è¯ï¼ˆåŒ…å«è‡ªå®šä¹‰è¯ï¼‰ã€‚
    2. è¿‡æ»¤æ‰åœç”¨è¯å’Œå•å­—ã€‚
    3. åšåŒä¹‰è¯å½’ä¸€åŒ–ã€‚
    """
    if not isinstance(doc_text, str):
        return []

    text = doc_text.lower()
    # ç§»é™¤æ ‡ç‚¹
    text = ''.join(c for c in text if c.isalnum() or c.isspace() or '\u4e00' <= c <= '\u9fa5')

    raw_tokens = list(jieba.cut(text))
    final_tokens = []

    for token in raw_tokens:
        token = token.strip()
        if len(token) > 1 and token not in STOP_WORDS:
            token = normalize_token(token)
            final_tokens.append(token)

    return final_tokens

def generate_simhash(doc_text):
    """
    ä½¿ç”¨è¯é¢‘ (TF) ä½œä¸ºæƒé‡ç”Ÿæˆ Simhash ç­¾åã€‚
    æ”¹è¿›ï¼šç­¾åé•¿åº¦ä» 64 ä½ -> 128 ä½ã€‚
    """
    tokens = get_tokens(doc_text)

    if not tokens:
        return Simhash([""], f=128)

    word_counts = Counter(tokens)
    weighted_tokens = [(token, count) for token, count in word_counts.items()]
    return Simhash(weighted_tokens, f=128)

def get_hamming_distance(hash1, hash2):
    """è®¡ç®—ä¸¤ä¸ª Simhash ç­¾åä¹‹é—´çš„æµ·æ˜è·ç¦»ã€‚"""
    if not (hash1 and hash2):
        return float('inf')
    return hash1.distance(hash2)

# ----------------------------------------------------------------------
if __name__ == '__main__':
    print("--- ğŸ”¬ Simhash æ–‡æ¡£ç›¸ä¼¼åº¦æ£€æµ‹æ¼”ç¤º (æ”¹è¿›ç‰ˆï¼šè‡ªå®šä¹‰è¯ + 128ä½) ---")

    # åˆå§‹åŒ–è‡ªå®šä¹‰è¯å…¸
    init_custom_words()

    # --- 1. å®šä¹‰æµ‹è¯•æ–‡æ¡£ ---
    doc_a = "å—äº¬å¤§å­¦çš„äººå·¥æ™ºèƒ½å­¦é™¢è‡´åŠ›äºè·¨å­¦ç§‘ç ”ç©¶ï¼Œèåˆäº†è®¡ç®—æœºç§‘å­¦ä¸ç¥ç»ç§‘å­¦çš„å‰æ²¿çŸ¥è¯†ï¼Œä»¥æ¨åŠ¨å“²å­¦å’Œç¤¾ä¼šå­¦é¢†åŸŸçš„å‘å±•ã€‚"
    doc_b = "å—äº¬å¤§å­¦çš„äººå·¥æ™ºèƒ½å­¦é™¢ä¸“æ³¨äºè·¨å­¦ç§‘æ¢ç´¢ï¼Œç»“åˆäº†è®¡ç®—æœºç§‘å­¦ä¸ç¥ç»ç§‘å­¦çš„æœ€æ–°çŸ¥è¯†ï¼Œä»¥æ¨åŠ¨å“²å­¦å’Œç¤¾ä¼šå­¦é¢†åŸŸçš„å‘å±•ã€‚"
    doc_c = "æ˜¨å¤œé›¨ç–é£éª¤ï¼Œæµ“ç¡ä¸æ¶ˆæ®‹é…’ã€‚è¯•é—®å·å¸˜äººï¼Œå´é“æµ·æ£ ä¾æ—§ã€‚çŸ¥å¦ï¼ŒçŸ¥å¦ï¼Ÿåº”æ˜¯ç»¿è‚¥çº¢ç˜¦ã€‚"

    # --- 2. ç”Ÿæˆ Simhash ç­¾å ---
    hash_a = generate_simhash(doc_a)
    hash_b = generate_simhash(doc_b)
    hash_c = generate_simhash(doc_c)

    # --- 3. è®¡ç®—æµ·æ˜è·ç¦» ---
    distance_ab = get_hamming_distance(hash_a, hash_b)
    distance_ac = get_hamming_distance(hash_a, hash_c)

    # --- 4. è¾“å‡ºç»“æœ ---
    print("\n--- ç›¸ä¼¼åº¦æ£€æµ‹ç»“æœ ---")
    print(f"æ–‡æ¡£ A (åŸå§‹): {doc_a[:20]}...")
    print(f"æ–‡æ¡£ B (ç›¸ä¼¼): {doc_b[:20]}...")
    print(f"æ–‡æ¡£ C (ä¸ç›¸ä¼¼): {doc_c[:20]}...")

    print(f"\n1. ç›¸ä¼¼æ–‡ç«  (A vs B) çš„æµ·æ˜è·ç¦»: {distance_ab}")
    print(f"   -> ç»“æœåˆ†æ: æµ·æ˜è·ç¦»ç›®æ ‡ **0-3**ï¼Œåˆ¤å®šä¸º**é«˜åº¦ç›¸ä¼¼/é‡å¤**ã€‚")

    print(f"2. ä¸ç›¸ä¼¼æ–‡ç«  (A vs C) çš„æµ·æ˜è·ç¦»: {distance_ac}")
    print(f"   -> ç»“æœåˆ†æ: è·ç¦»ä¿æŒè¾ƒå¤§ï¼Œåˆ¤å®šä¸º**ä¸åŒä¸»é¢˜**ã€‚")
